---------------------------------------------------------------------------
Training stage 0
Sampled 57982 windows from 6334 images.
Done sampling windows (time=133s).
Computing lambdas... done (time=131s).
Extracting features... done (time=49s).
Sampled 15000 windows from 640 images.
Done sampling windows (time=14s).
Extracting features... done (time=6s).
Training AdaBoost: nWeak= 32 nFtrs=2600 pos=115964 neg=15000
 i=  16 alpha=1.000 err=0.304 loss=6.39e-02
 i=  32 alpha=1.000 err=0.309 loss=1.20e-02
Done training err=0.0011 fp=0.0002 fn=0.0020 (t=6.4s).
Done training stage 0 (time=347s).
---------------------------------------------------------------------------
Training stage 1
Sampled 15000 windows from 960 images.
Done sampling windows (time=31s).
Extracting features... done (time=9s).
Training AdaBoost: nWeak=128 nFtrs=2600 pos=115964 neg=25000
 i=  16 alpha=1.000 err=0.369 loss=2.72e-01
 i=  32 alpha=1.000 err=0.372 loss=1.37e-01
 i=  48 alpha=1.000 err=0.363 loss=7.53e-02
 i=  64 alpha=1.000 err=0.370 loss=4.04e-02
 i=  80 alpha=1.000 err=0.371 loss=2.26e-02
 i=  96 alpha=1.000 err=0.376 loss=1.21e-02
 i= 112 alpha=1.000 err=0.379 loss=6.62e-03
 i= 128 alpha=1.000 err=0.372 loss=3.63e-03
Done training err=0.0000 fp=0.0000 fn=0.0000 (t=18.8s).
Done training stage 1 (time=60s).
---------------------------------------------------------------------------
Training stage 2
Sampled 11099 windows from 1245 images.
Done sampling windows (time=32s).
Extracting features... done (time=4s).
Training AdaBoost: nWeak=512 nFtrs=2600 pos=115964 neg=25000
 i=  16 alpha=1.000 err=0.379 loss=3.80e-01
 i=  32 alpha=1.000 err=0.393 loss=2.27e-01
 i=  48 alpha=1.000 err=0.395 loss=1.48e-01
 i=  64 alpha=1.000 err=0.403 loss=9.90e-02
 i=  80 alpha=1.000 err=0.388 loss=6.36e-02
 i=  96 alpha=1.000 err=0.403 loss=4.17e-02
 i= 112 alpha=1.000 err=0.394 loss=2.69e-02
 i= 128 alpha=1.000 err=0.401 loss=1.73e-02
 i= 144 alpha=1.000 err=0.396 loss=1.13e-02
 i= 160 alpha=1.000 err=0.396 loss=7.45e-03
 i= 176 alpha=1.000 err=0.390 loss=4.88e-03
 i= 192 alpha=1.000 err=0.397 loss=3.21e-03
 i= 208 alpha=1.000 err=0.390 loss=2.13e-03
 i= 224 alpha=1.000 err=0.388 loss=1.38e-03
 i= 240 alpha=1.000 err=0.396 loss=9.01e-04
 i= 256 alpha=1.000 err=0.406 loss=5.84e-04
 i= 272 alpha=1.000 err=0.397 loss=3.73e-04
 i= 288 alpha=1.000 err=0.407 loss=2.44e-04
 i= 304 alpha=1.000 err=0.377 loss=1.61e-04
 i= 320 alpha=1.000 err=0.383 loss=1.03e-04
 i= 336 alpha=1.000 err=0.399 loss=6.71e-05
 i= 352 alpha=1.000 err=0.400 loss=4.47e-05
 i= 368 alpha=1.000 err=0.406 loss=2.96e-05
 i= 384 alpha=1.000 err=0.392 loss=1.93e-05
 i= 400 alpha=1.000 err=0.393 loss=1.26e-05
 i= 416 alpha=1.000 err=0.399 loss=8.22e-06
 i= 432 alpha=1.000 err=0.386 loss=5.32e-06
 i= 448 alpha=1.000 err=0.395 loss=3.47e-06
 i= 464 alpha=1.000 err=0.392 loss=2.24e-06
 i= 480 alpha=1.000 err=0.385 loss=1.45e-06
 i= 496 alpha=1.000 err=0.399 loss=9.59e-07
 i= 512 alpha=1.000 err=0.393 loss=6.21e-07
Done training err=0.0000 fp=0.0000 fn=0.0000 (t=60.7s).
Done training stage 2 (time=98s).
---------------------------------------------------------------------------
Training stage 3
Sampled 3566 windows from 1245 images.
Done sampling windows (time=32s).
Extracting features... done (time=2s).
Training AdaBoost: nWeak=2048 nFtrs=2600 pos=115964 neg=25000
 i=  16 alpha=1.000 err=0.371 loss=4.25e-01
 i=  32 alpha=1.000 err=0.405 loss=2.85e-01
 i=  48 alpha=1.000 err=0.411 loss=1.99e-01
 i=  64 alpha=1.000 err=0.402 loss=1.37e-01
 i=  80 alpha=1.000 err=0.396 loss=9.48e-02
 i=  96 alpha=1.000 err=0.410 loss=6.53e-02
 i= 112 alpha=1.000 err=0.400 loss=4.59e-02
 i= 128 alpha=1.000 err=0.394 loss=3.25e-02
 i= 144 alpha=1.000 err=0.398 loss=2.27e-02
 i= 160 alpha=1.000 err=0.407 loss=1.58e-02
 i= 176 alpha=1.000 err=0.401 loss=1.11e-02
 i= 192 alpha=1.000 err=0.408 loss=7.71e-03
 i= 208 alpha=1.000 err=0.398 loss=5.47e-03
 i= 224 alpha=1.000 err=0.404 loss=3.84e-03
 i= 240 alpha=1.000 err=0.401 loss=2.72e-03
 i= 256 alpha=1.000 err=0.408 loss=1.90e-03
 i= 272 alpha=1.000 err=0.411 loss=1.35e-03
 i= 288 alpha=1.000 err=0.405 loss=9.52e-04
 i= 304 alpha=1.000 err=0.405 loss=6.56e-04
 i= 320 alpha=1.000 err=0.410 loss=4.61e-04
 i= 336 alpha=1.000 err=0.400 loss=3.20e-04
 i= 352 alpha=1.000 err=0.421 loss=2.26e-04
 i= 368 alpha=1.000 err=0.398 loss=1.55e-04
 i= 384 alpha=1.000 err=0.402 loss=1.06e-04
 i= 400 alpha=1.000 err=0.409 loss=7.30e-05
 i= 416 alpha=1.000 err=0.414 loss=5.20e-05
 i= 432 alpha=1.000 err=0.407 loss=3.64e-05
 i= 448 alpha=1.000 err=0.411 loss=2.58e-05
 i= 464 alpha=1.000 err=0.402 loss=1.80e-05
 i= 480 alpha=1.000 err=0.424 loss=1.27e-05
 i= 496 alpha=1.000 err=0.419 loss=8.75e-06
 i= 512 alpha=1.000 err=0.389 loss=6.01e-06
 i= 528 alpha=1.000 err=0.410 loss=4.20e-06
 i= 544 alpha=1.000 err=0.407 loss=2.96e-06
 i= 560 alpha=1.000 err=0.408 loss=2.00e-06
 i= 576 alpha=1.000 err=0.410 loss=1.39e-06
 i= 592 alpha=1.000 err=0.412 loss=9.49e-07
 i= 608 alpha=1.000 err=0.406 loss=6.51e-07
 i= 624 alpha=1.000 err=0.402 loss=4.48e-07
 i= 640 alpha=1.000 err=0.411 loss=3.13e-07
 i= 656 alpha=1.000 err=0.398 loss=2.22e-07
 i= 672 alpha=1.000 err=0.410 loss=1.55e-07
 i= 688 alpha=1.000 err=0.397 loss=1.07e-07
 i= 704 alpha=1.000 err=0.414 loss=7.51e-08
 i= 720 alpha=1.000 err=0.413 loss=5.27e-08
 i= 736 alpha=1.000 err=0.402 loss=3.73e-08
 i= 752 alpha=1.000 err=0.401 loss=2.64e-08
 i= 768 alpha=1.000 err=0.403 loss=1.82e-08
 i= 784 alpha=1.000 err=0.411 loss=1.27e-08
 i= 800 alpha=1.000 err=0.394 loss=8.86e-09
 i= 816 alpha=1.000 err=0.402 loss=6.26e-09
 i= 832 alpha=1.000 err=0.405 loss=4.37e-09
 i= 848 alpha=1.000 err=0.397 loss=3.06e-09
 i= 864 alpha=1.000 err=0.394 loss=2.14e-09
 i= 880 alpha=1.000 err=0.410 loss=1.51e-09
 i= 896 alpha=1.000 err=0.395 loss=1.03e-09
 i= 912 alpha=1.000 err=0.414 loss=7.05e-10
 i= 928 alpha=1.000 err=0.406 loss=4.85e-10
 i= 944 alpha=1.000 err=0.402 loss=3.35e-10
 i= 960 alpha=1.000 err=0.404 loss=2.36e-10
 i= 976 alpha=1.000 err=0.402 loss=1.64e-10
 i= 992 alpha=1.000 err=0.412 loss=1.11e-10
 i=1008 alpha=1.000 err=0.398 loss=7.66e-11
 i=1024 alpha=1.000 err=0.410 loss=5.40e-11
 i=1040 alpha=1.000 err=0.404 loss=3.82e-11
 i=1056 alpha=1.000 err=0.403 loss=2.67e-11
 i=1072 alpha=1.000 err=0.402 loss=1.86e-11
 i=1088 alpha=1.000 err=0.400 loss=1.27e-11
 i=1104 alpha=1.000 err=0.403 loss=8.73e-12
 i=1120 alpha=1.000 err=0.406 loss=6.10e-12
 i=1136 alpha=1.000 err=0.390 loss=4.18e-12
 i=1152 alpha=1.000 err=0.398 loss=2.87e-12
 i=1168 alpha=1.000 err=0.407 loss=1.99e-12
 i=1184 alpha=1.000 err=0.403 loss=1.39e-12
 i=1200 alpha=1.000 err=0.406 loss=9.78e-13
 i=1216 alpha=1.000 err=0.406 loss=6.87e-13
 i=1232 alpha=1.000 err=0.406 loss=4.77e-13
 i=1248 alpha=1.000 err=0.402 loss=3.29e-13
 i=1264 alpha=1.000 err=0.397 loss=2.25e-13
 i=1280 alpha=1.000 err=0.401 loss=1.54e-13
 i=1296 alpha=1.000 err=0.408 loss=1.07e-13
 i=1312 alpha=1.000 err=0.409 loss=7.39e-14
 i=1328 alpha=1.000 err=0.401 loss=5.15e-14
 i=1344 alpha=1.000 err=0.400 loss=3.57e-14
 i=1360 alpha=1.000 err=0.402 loss=2.47e-14
 i=1376 alpha=1.000 err=0.402 loss=1.70e-14
 i=1392 alpha=1.000 err=0.404 loss=1.17e-14
 i=1408 alpha=1.000 err=0.403 loss=8.22e-15
 i=1424 alpha=1.000 err=0.411 loss=5.70e-15
 i=1440 alpha=1.000 err=0.399 loss=3.96e-15
 i=1456 alpha=1.000 err=0.416 loss=2.77e-15
 i=1472 alpha=1.000 err=0.400 loss=1.90e-15
 i=1488 alpha=1.000 err=0.404 loss=1.35e-15
 i=1504 alpha=1.000 err=0.407 loss=9.29e-16
 i=1520 alpha=1.000 err=0.409 loss=6.49e-16
 i=1536 alpha=1.000 err=0.401 loss=4.48e-16
 i=1552 alpha=1.000 err=0.406 loss=3.13e-16
 i=1568 alpha=1.000 err=0.395 loss=2.19e-16
 i=1584 alpha=1.000 err=0.402 loss=1.49e-16
 i=1600 alpha=1.000 err=0.406 loss=1.04e-16
 i=1616 alpha=1.000 err=0.396 loss=7.13e-17
 i=1632 alpha=1.000 err=0.411 loss=5.00e-17
 i=1648 alpha=1.000 err=0.405 loss=3.50e-17
 i=1664 alpha=1.000 err=0.407 loss=2.47e-17
 i=1680 alpha=1.000 err=0.408 loss=1.73e-17
 i=1696 alpha=1.000 err=0.410 loss=1.22e-17
 i=1712 alpha=1.000 err=0.411 loss=8.27e-18
 i=1728 alpha=1.000 err=0.406 loss=5.70e-18
 i=1744 alpha=1.000 err=0.406 loss=3.96e-18
 i=1760 alpha=1.000 err=0.401 loss=2.75e-18
 i=1776 alpha=1.000 err=0.408 loss=1.90e-18
 i=1792 alpha=1.000 err=0.406 loss=1.30e-18
 i=1808 alpha=1.000 err=0.401 loss=8.92e-19
 i=1824 alpha=1.000 err=0.400 loss=6.17e-19
 i=1840 alpha=1.000 err=0.407 loss=4.44e-19
 i=1856 alpha=1.000 err=0.397 loss=3.05e-19
 i=1872 alpha=1.000 err=0.410 loss=2.09e-19
 i=1888 alpha=1.000 err=0.395 loss=1.46e-19
 i=1904 alpha=1.000 err=0.392 loss=1.01e-19
 i=1920 alpha=1.000 err=0.408 loss=6.93e-20
 i=1936 alpha=1.000 err=0.393 loss=4.80e-20
 i=1952 alpha=1.000 err=0.409 loss=3.35e-20
 i=1968 alpha=1.000 err=0.403 loss=2.32e-20
 i=1984 alpha=1.000 err=0.401 loss=1.62e-20
 i=2000 alpha=1.000 err=0.409 loss=1.14e-20
 i=2016 alpha=1.000 err=0.404 loss=7.75e-21
 i=2032 alpha=1.000 err=0.404 loss=5.33e-21
 i=2048 alpha=1.000 err=0.410 loss=3.74e-21
Done training err=0.0000 fp=0.0000 fn=0.0000 (t=226.8s).
Done training stage 3 (time=262s).
---------------------------------------------------------------------------
Done training (time=767s).
