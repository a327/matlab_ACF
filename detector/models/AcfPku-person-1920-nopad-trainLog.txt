---------------------------------------------------------------------------
Training stage 0
Sampled 34426 windows from 6334 images.
Done sampling windows (time=154s).
Computing lambdas... done (time=124s).
Extracting features... done (time=114s).
Sampled 15000 windows from 640 images.
Done sampling windows (time=19s).
Extracting features... done (time=19s).
Training AdaBoost: nWeak= 32 nFtrs=24900 pos=68852 neg=15000
 i=  16 alpha=1.000 err=0.264 loss=4.62e-02
 i=  32 alpha=1.000 err=0.248 loss=5.29e-03
Done training err=0.0001 fp=0.0000 fn=0.0002 (t=23.1s).
Done training stage 0 (time=472s).
---------------------------------------------------------------------------
Training stage 1
Sampled 11104 windows from 1245 images.
Done sampling windows (time=44s).
Extracting features... done (time=13s).
Training AdaBoost: nWeak=128 nFtrs=24900 pos=68852 neg=25000
 i=  16 alpha=1.000 err=0.304 loss=1.36e-01
 i=  32 alpha=1.000 err=0.331 loss=4.05e-02
 i=  48 alpha=1.000 err=0.329 loss=1.18e-02
 i=  64 alpha=1.000 err=0.309 loss=3.25e-03
 i=  80 alpha=1.000 err=0.334 loss=8.61e-04
 i=  96 alpha=1.000 err=0.324 loss=2.30e-04
 i= 112 alpha=1.000 err=0.323 loss=6.54e-05
 i= 128 alpha=1.000 err=0.331 loss=1.85e-05
Done training err=0.0000 fp=0.0000 fn=0.0000 (t=63.9s).
Done training stage 1 (time=127s).
---------------------------------------------------------------------------
Training stage 2
Sampled 2952 windows from 1245 images.
Done sampling windows (time=38s).
Extracting features... done (time=5s).
Training AdaBoost: nWeak=512 nFtrs=24900 pos=68852 neg=25000
 i=  16 alpha=1.000 err=0.338 loss=1.68e-01
 i=  32 alpha=1.000 err=0.319 loss=5.70e-02
 i=  48 alpha=1.000 err=0.317 loss=1.93e-02
 i=  64 alpha=1.000 err=0.314 loss=6.35e-03
 i=  80 alpha=1.000 err=0.347 loss=2.19e-03
 i=  96 alpha=1.000 err=0.339 loss=7.65e-04
 i= 112 alpha=1.000 err=0.350 loss=2.55e-04
 i= 128 alpha=1.000 err=0.329 loss=8.32e-05
 i= 144 alpha=1.000 err=0.337 loss=2.64e-05
 i= 160 alpha=1.000 err=0.329 loss=8.66e-06
 i= 176 alpha=1.000 err=0.335 loss=2.83e-06
 i= 192 alpha=1.000 err=0.329 loss=9.30e-07
 i= 208 alpha=1.000 err=0.327 loss=3.12e-07
 i= 224 alpha=1.000 err=0.349 loss=1.05e-07
 i= 240 alpha=1.000 err=0.319 loss=3.36e-08
 i= 256 alpha=1.000 err=0.356 loss=1.05e-08
 i= 272 alpha=1.000 err=0.339 loss=3.53e-09
 i= 288 alpha=1.000 err=0.345 loss=1.14e-09
 i= 304 alpha=1.000 err=0.330 loss=3.66e-10
 i= 320 alpha=1.000 err=0.316 loss=1.17e-10
 i= 336 alpha=1.000 err=0.342 loss=3.80e-11
 i= 352 alpha=1.000 err=0.340 loss=1.22e-11
 i= 368 alpha=1.000 err=0.326 loss=4.11e-12
 i= 384 alpha=1.000 err=0.342 loss=1.29e-12
 i= 400 alpha=1.000 err=0.336 loss=4.24e-13
 i= 416 alpha=1.000 err=0.325 loss=1.39e-13
 i= 432 alpha=1.000 err=0.339 loss=4.52e-14
 i= 448 alpha=1.000 err=0.333 loss=1.45e-14
 i= 464 alpha=1.000 err=0.338 loss=4.78e-15
 i= 480 alpha=1.000 err=0.347 loss=1.55e-15
 i= 496 alpha=1.000 err=0.307 loss=4.99e-16
 i= 512 alpha=1.000 err=0.321 loss=1.56e-16
Done training err=0.0000 fp=0.0000 fn=0.0000 (t=213.7s).
Done training stage 2 (time=261s).
---------------------------------------------------------------------------
Training stage 3
Sampled 659 windows from 1245 images.
Done sampling windows (time=34s).
Extracting features... done (time=1s).
Training AdaBoost: nWeak=2048 nFtrs=24900 pos=68852 neg=25000
 i=  16 alpha=1.000 err=0.330 loss=1.98e-01
 i=  32 alpha=1.000 err=0.354 loss=6.81e-02
 i=  48 alpha=1.000 err=0.345 loss=2.52e-02
 i=  64 alpha=1.000 err=0.334 loss=9.45e-03
 i=  80 alpha=1.000 err=0.340 loss=3.39e-03
 i=  96 alpha=1.000 err=0.315 loss=1.18e-03
 i= 112 alpha=1.000 err=0.338 loss=4.36e-04
 i= 128 alpha=1.000 err=0.321 loss=1.58e-04
 i= 144 alpha=1.000 err=0.348 loss=5.56e-05
 i= 160 alpha=1.000 err=0.346 loss=2.05e-05
 i= 176 alpha=1.000 err=0.357 loss=7.09e-06
 i= 192 alpha=1.000 err=0.342 loss=2.49e-06
 i= 208 alpha=1.000 err=0.327 loss=8.86e-07
 i= 224 alpha=1.000 err=0.348 loss=3.16e-07
 i= 240 alpha=1.000 err=0.351 loss=1.13e-07
 i= 256 alpha=1.000 err=0.331 loss=3.80e-08
 i= 272 alpha=1.000 err=0.336 loss=1.35e-08
 i= 288 alpha=1.000 err=0.337 loss=4.67e-09
 i= 304 alpha=1.000 err=0.336 loss=1.55e-09
 i= 320 alpha=1.000 err=0.342 loss=5.45e-10
 i= 336 alpha=1.000 err=0.330 loss=1.83e-10
 i= 352 alpha=1.000 err=0.329 loss=6.21e-11
 i= 368 alpha=1.000 err=0.350 loss=2.20e-11
 i= 384 alpha=1.000 err=0.340 loss=7.63e-12
 i= 400 alpha=1.000 err=0.335 loss=2.66e-12
 i= 416 alpha=1.000 err=0.333 loss=8.63e-13
 i= 432 alpha=1.000 err=0.346 loss=2.99e-13
 i= 448 alpha=1.000 err=0.342 loss=1.01e-13
 i= 464 alpha=1.000 err=0.340 loss=3.52e-14
 i= 480 alpha=1.000 err=0.352 loss=1.24e-14
 i= 496 alpha=1.000 err=0.313 loss=4.13e-15
 i= 512 alpha=1.000 err=0.336 loss=1.43e-15
 i= 528 alpha=1.000 err=0.334 loss=4.87e-16
 i= 544 alpha=1.000 err=0.346 loss=1.62e-16
 i= 560 alpha=1.000 err=0.336 loss=5.64e-17
 i= 576 alpha=1.000 err=0.334 loss=1.82e-17
 i= 592 alpha=1.000 err=0.322 loss=6.18e-18
 i= 608 alpha=1.000 err=0.332 loss=2.05e-18
 i= 624 alpha=1.000 err=0.338 loss=7.28e-19
 i= 640 alpha=1.000 err=0.353 loss=2.55e-19
 i= 656 alpha=1.000 err=0.348 loss=8.80e-20
 i= 672 alpha=1.000 err=0.332 loss=3.02e-20
 i= 688 alpha=1.000 err=0.348 loss=9.98e-21
 i= 704 alpha=1.000 err=0.348 loss=3.31e-21
 i= 720 alpha=1.000 err=0.346 loss=1.11e-21
 i= 736 alpha=1.000 err=0.337 loss=3.86e-22
 i= 752 alpha=1.000 err=0.333 loss=1.31e-22
 i= 768 alpha=1.000 err=0.337 loss=4.48e-23
 i= 784 alpha=1.000 err=0.341 loss=1.51e-23
 i= 800 alpha=1.000 err=0.327 loss=5.20e-24
 i= 816 alpha=1.000 err=0.333 loss=1.84e-24
 i= 832 alpha=1.000 err=0.326 loss=6.27e-25
 i= 848 alpha=1.000 err=0.326 loss=2.11e-25
 i= 864 alpha=1.000 err=0.332 loss=7.19e-26
 i= 880 alpha=1.000 err=0.333 loss=2.51e-26
 i= 896 alpha=1.000 err=0.328 loss=8.68e-27
 i= 912 alpha=1.000 err=0.352 loss=3.02e-27
 i= 928 alpha=1.000 err=0.331 loss=1.07e-27
 i= 944 alpha=1.000 err=0.325 loss=3.67e-28
 i= 960 alpha=1.000 err=0.351 loss=1.32e-28
 i= 976 alpha=1.000 err=0.331 loss=4.41e-29
 i= 992 alpha=1.000 err=0.319 loss=1.47e-29
 i=1008 alpha=1.000 err=0.336 loss=5.05e-30
 i=1024 alpha=1.000 err=0.326 loss=1.78e-30
 i=1040 alpha=1.000 err=0.332 loss=6.15e-31
 i=1056 alpha=1.000 err=0.362 loss=2.18e-31
 i=1072 alpha=1.000 err=0.341 loss=7.63e-32
 i=1088 alpha=1.000 err=0.328 loss=2.68e-32
 i=1104 alpha=1.000 err=0.332 loss=8.52e-33
 i=1120 alpha=1.000 err=0.347 loss=2.78e-33
 i=1136 alpha=1.000 err=0.344 loss=9.50e-34
 i=1152 alpha=1.000 err=0.326 loss=3.27e-34
 i=1168 alpha=1.000 err=0.323 loss=1.10e-34
 i=1184 alpha=1.000 err=0.332 loss=3.84e-35
 i=1200 alpha=1.000 err=0.315 loss=1.35e-35
 i=1216 alpha=1.000 err=0.330 loss=4.54e-36
 i=1232 alpha=1.000 err=0.331 loss=1.48e-36
 i=1248 alpha=1.000 err=0.358 loss=5.15e-37
 i=1264 alpha=1.000 err=0.327 loss=1.74e-37
 i=1280 alpha=1.000 err=0.348 loss=5.69e-38
 i=1296 alpha=1.000 err=0.344 loss=2.00e-38
 i=1312 alpha=1.000 err=0.333 loss=6.84e-39
 i=1328 alpha=1.000 err=0.351 loss=2.29e-39
 i=1344 alpha=1.000 err=0.326 loss=7.52e-40
 i=1360 alpha=1.000 err=0.334 loss=2.57e-40
 stopping early
Done training err=0.0000 fp=0.0000 fn=0.0000 (t=551.8s).
Done training stage 3 (time=591s).
---------------------------------------------------------------------------
Done training (time=1450s).
